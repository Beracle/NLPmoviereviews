{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŽ¯ The goal of this challenge is to find topics within a corpus of reviews with the **LDA** algorithm (Unsupervised Learning in NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "  \n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "    \n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "    \n",
    "    X_train = [' '.join(_) for _ in X_train]\n",
    "    X_test = [' '.join(_) for _ in X_test]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this was an absolutely terrible movie don't be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have been known to fall asleep during films ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mann photographs the alberta rocky mountains i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is the kind of film for a snowy sunday af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as others have mentioned all the women that go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  this was an absolutely terrible movie don't be...\n",
       "1  i have been known to fall asleep during films ...\n",
       "2  mann photographs the alberta rocky mountains i...\n",
       "3  this is the kind of film for a snowy sunday af...\n",
       "4  as others have mentioned all the women that go..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(X_train)\n",
    "data.columns = ['text']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the text with **NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    # Basic cleaning\n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.lower() ## lowercasing \n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## removing numbers\n",
    "    # Advanced cleaning\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') ## removing punctuation\n",
    "    tokenized_sentence = word_tokenize(sentence) ## tokenizing \n",
    "    stop_words = set(stopwords.words('english')) ## defining stopwords\n",
    "    tokenized_sentence_cleaned = [w for w in tokenized_sentence \n",
    "                                  if not w in stop_words] ## remove stopwords\n",
    "    # 1 - Lemmatizing the verbs\n",
    "    verb_lemmatized = [WordNetLemmatizer().lemmatize(word, pos = \"v\")  # v --> verbs\n",
    "              for word in tokenized_sentence_cleaned]\n",
    "    # 2 - Lemmatizing the nouns\n",
    "    noun_lemmatized = [WordNetLemmatizer().lemmatize(word, pos = \"n\")  # n --> nouns\n",
    "                for word in verb_lemmatized]\n",
    "    cleaned_sentence= ' '.join(w for w in noun_lemmatized)\n",
    "    return cleaned_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this was an absolutely terrible movie don't be...</td>\n",
       "      <td>absolutely terrible movie dont lure christophe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have been known to fall asleep during films ...</td>\n",
       "      <td>know fall asleep film usually due combination ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mann photographs the alberta rocky mountains i...</td>\n",
       "      <td>mann photograph alberta rocky mountain superb ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is the kind of film for a snowy sunday af...</td>\n",
       "      <td>kind film snowy sunday afternoon rest world go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as others have mentioned all the women that go...</td>\n",
       "      <td>others mention woman go nude film mostly absol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>bad acting bad writing this was a poorly writt...</td>\n",
       "      <td>bad act bad write poorly write film bad potent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>when i saw that imdb users rated this movie th...</td>\n",
       "      <td>saw imdb user rat movie bottom movie think har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>there is something about pet sematary that i n...</td>\n",
       "      <td>something pet sematary never felt anywhere els...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>what the hell of a d movie was that bad acting...</td>\n",
       "      <td>hell movie bad act bad special effect worst di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>this movie is a perfect adaptation of the engl...</td>\n",
       "      <td>movie perfect adaptation english flick unfaith...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     this was an absolutely terrible movie don't be...   \n",
       "1     i have been known to fall asleep during films ...   \n",
       "2     mann photographs the alberta rocky mountains i...   \n",
       "3     this is the kind of film for a snowy sunday af...   \n",
       "4     as others have mentioned all the women that go...   \n",
       "...                                                 ...   \n",
       "2495  bad acting bad writing this was a poorly writt...   \n",
       "2496  when i saw that imdb users rated this movie th...   \n",
       "2497  there is something about pet sematary that i n...   \n",
       "2498  what the hell of a d movie was that bad acting...   \n",
       "2499  this movie is a perfect adaptation of the engl...   \n",
       "\n",
       "                                             clean_text  \n",
       "0     absolutely terrible movie dont lure christophe...  \n",
       "1     know fall asleep film usually due combination ...  \n",
       "2     mann photograph alberta rocky mountain superb ...  \n",
       "3     kind film snowy sunday afternoon rest world go...  \n",
       "4     others mention woman go nude film mostly absol...  \n",
       "...                                                 ...  \n",
       "2495  bad act bad write poorly write film bad potent...  \n",
       "2496  saw imdb user rat movie bottom movie think har...  \n",
       "2497  something pet sematary never felt anywhere els...  \n",
       "2498  hell movie bad act bad special effect worst di...  \n",
       "2499  movie perfect adaptation english flick unfaith...  \n",
       "\n",
       "[2500 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text']=data.text.apply(preprocessing)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Latent Dirichlet Allocation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a LDA model to extract potential topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aag</th>\n",
       "      <th>aames</th>\n",
       "      <th>aapke</th>\n",
       "      <th>aardman</th>\n",
       "      <th>aargh</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aatish</th>\n",
       "      <th>ab</th>\n",
       "      <th>...</th>\n",
       "      <th>zyuranger</th>\n",
       "      <th>zz</th>\n",
       "      <th>zzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzz</th>\n",
       "      <th>zÃ©</th>\n",
       "      <th>Ã¨me</th>\n",
       "      <th>Ã©lan</th>\n",
       "      <th>Ã©migrÃ©</th>\n",
       "      <th>Ã©tait</th>\n",
       "      <th>Ã©tc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows Ã— 21727 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaaaah  aaah  aag  aames  aapke  aardman  aargh  aaron  aatish   ab  \\\n",
       "0        0.0   0.0  0.0    0.0    0.0      0.0    0.0    0.0     0.0  0.0   \n",
       "1        0.0   0.0  0.0    0.0    0.0      0.0    0.0    0.0     0.0  0.0   \n",
       "2        0.0   0.0  0.0    0.0    0.0      0.0    0.0    0.0     0.0  0.0   \n",
       "3        0.0   0.0  0.0    0.0    0.0      0.0    0.0    0.0     0.0  0.0   \n",
       "4        0.0   0.0  0.0    0.0    0.0      0.0    0.0    0.0     0.0  0.0   \n",
       "...      ...   ...  ...    ...    ...      ...    ...    ...     ...  ...   \n",
       "2495     0.0   0.0  0.0    0.0    0.0      0.0    0.0    0.0     0.0  0.0   \n",
       "2496     0.0   0.0  0.0    0.0    0.0      0.0    0.0    0.0     0.0  0.0   \n",
       "2497     0.0   0.0  0.0    0.0    0.0      0.0    0.0    0.0     0.0  0.0   \n",
       "2498     0.0   0.0  0.0    0.0    0.0      0.0    0.0    0.0     0.0  0.0   \n",
       "2499     0.0   0.0  0.0    0.0    0.0      0.0    0.0    0.0     0.0  0.0   \n",
       "\n",
       "      ...  zyuranger   zz  zzzzzzzzzzzz  zzzzzzzzzzzzz   zÃ©  Ã¨me  Ã©lan  \\\n",
       "0     ...        0.0  0.0           0.0            0.0  0.0  0.0   0.0   \n",
       "1     ...        0.0  0.0           0.0            0.0  0.0  0.0   0.0   \n",
       "2     ...        0.0  0.0           0.0            0.0  0.0  0.0   0.0   \n",
       "3     ...        0.0  0.0           0.0            0.0  0.0  0.0   0.0   \n",
       "4     ...        0.0  0.0           0.0            0.0  0.0  0.0   0.0   \n",
       "...   ...        ...  ...           ...            ...  ...  ...   ...   \n",
       "2495  ...        0.0  0.0           0.0            0.0  0.0  0.0   0.0   \n",
       "2496  ...        0.0  0.0           0.0            0.0  0.0  0.0   0.0   \n",
       "2497  ...        0.0  0.0           0.0            0.0  0.0  0.0   0.0   \n",
       "2498  ...        0.0  0.0           0.0            0.0  0.0  0.0   0.0   \n",
       "2499  ...        0.0  0.0           0.0            0.0  0.0  0.0   0.0   \n",
       "\n",
       "      Ã©migrÃ©  Ã©tait  Ã©tc  \n",
       "0        0.0    0.0  0.0  \n",
       "1        0.0    0.0  0.0  \n",
       "2        0.0    0.0  0.0  \n",
       "3        0.0    0.0  0.0  \n",
       "4        0.0    0.0  0.0  \n",
       "...      ...    ...  ...  \n",
       "2495     0.0    0.0  0.0  \n",
       "2496     0.0    0.0  0.0  \n",
       "2497     0.0    0.0  0.0  \n",
       "2498     0.0    0.0  0.0  \n",
       "2499     0.0    0.0  0.0  \n",
       "\n",
       "[2500 rows x 21727 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "vectorizer = TfidfVectorizer().fit(data.clean_text)\n",
    "vectorized_text = vectorizer.transform(data.clean_text)\n",
    "vectorized_text = pd.DataFrame(vectorized_text.toarray(), columns=vectorizer.get_feature_names())\n",
    "vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=100,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating the LDA \n",
    "n_components = 5\n",
    "lda_model = LatentDirichletAllocation(n_components=n_components, max_iter = 100)\n",
    "\n",
    "# Fitting the LDA on the vectorized documents\n",
    "lda_model.fit(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025802</td>\n",
       "      <td>0.025802</td>\n",
       "      <td>0.896790</td>\n",
       "      <td>0.025802</td>\n",
       "      <td>0.025803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027351</td>\n",
       "      <td>0.027351</td>\n",
       "      <td>0.890598</td>\n",
       "      <td>0.027351</td>\n",
       "      <td>0.027351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.024156</td>\n",
       "      <td>0.024156</td>\n",
       "      <td>0.903376</td>\n",
       "      <td>0.024156</td>\n",
       "      <td>0.024156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.027315</td>\n",
       "      <td>0.027315</td>\n",
       "      <td>0.890742</td>\n",
       "      <td>0.027315</td>\n",
       "      <td>0.027315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.032707</td>\n",
       "      <td>0.032707</td>\n",
       "      <td>0.869172</td>\n",
       "      <td>0.032707</td>\n",
       "      <td>0.032707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>0.034788</td>\n",
       "      <td>0.034788</td>\n",
       "      <td>0.860846</td>\n",
       "      <td>0.034788</td>\n",
       "      <td>0.034788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>0.021930</td>\n",
       "      <td>0.021930</td>\n",
       "      <td>0.912279</td>\n",
       "      <td>0.021930</td>\n",
       "      <td>0.021930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>0.024466</td>\n",
       "      <td>0.024466</td>\n",
       "      <td>0.902137</td>\n",
       "      <td>0.024466</td>\n",
       "      <td>0.024466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>0.029902</td>\n",
       "      <td>0.029902</td>\n",
       "      <td>0.880393</td>\n",
       "      <td>0.029902</td>\n",
       "      <td>0.029902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>0.023013</td>\n",
       "      <td>0.023013</td>\n",
       "      <td>0.907947</td>\n",
       "      <td>0.023013</td>\n",
       "      <td>0.023013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4\n",
       "0     0.025802  0.025802  0.896790  0.025802  0.025803\n",
       "1     0.027351  0.027351  0.890598  0.027351  0.027351\n",
       "2     0.024156  0.024156  0.903376  0.024156  0.024156\n",
       "3     0.027315  0.027315  0.890742  0.027315  0.027315\n",
       "4     0.032707  0.032707  0.869172  0.032707  0.032707\n",
       "...        ...       ...       ...       ...       ...\n",
       "2495  0.034788  0.034788  0.860846  0.034788  0.034788\n",
       "2496  0.021930  0.021930  0.912279  0.021930  0.021930\n",
       "2497  0.024466  0.024466  0.902137  0.024466  0.024466\n",
       "2498  0.029902  0.029902  0.880393  0.029902  0.029902\n",
       "2499  0.023013  0.023013  0.907947  0.023013  0.023013\n",
       "\n",
       "[2500 rows x 5 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_topics=lda_model.transform(vectorized_text)\n",
    "pd.DataFrame(text_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  (3) Visualize potential topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, vectorizer):\n",
    "    topic_mixture = pd.DataFrame(lda_model.components_,\n",
    "                                 columns = vectorizer.get_feature_names())\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        topic_df = topic_mixture.iloc[idx].sort_values(ascending = False).head(3)\n",
    "        print(round(topic_df,3))\n",
    "        print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the topics extracted by your LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "scanner    1.036\n",
      "cognac     0.981\n",
      "hou        0.945\n",
      "Name: 0, dtype: float64\n",
      "----------\n",
      "Topic 1:\n",
      "kamal     1.142\n",
      "salman    1.073\n",
      "rani      0.979\n",
      "Name: 1, dtype: float64\n",
      "----------\n",
      "Topic 2:\n",
      "br       240.958\n",
      "movie    139.708\n",
      "film     120.632\n",
      "Name: 2, dtype: float64\n",
      "----------\n",
      "Topic 3:\n",
      "blackadder    0.827\n",
      "ollie         0.804\n",
      "cruella       0.777\n",
      "Name: 3, dtype: float64\n",
      "----------\n",
      "Topic 4:\n",
      "pokÃ©mon     0.900\n",
      "kangwon     0.645\n",
      "province    0.645\n",
      "Name: 4, dtype: float64\n",
      "----------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(print_topics(lda_model,vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Predict the document-topic mixture of a new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the LDA model is fitted, we can use it to predict the topics of a new text.\n",
    "\n",
    "1. Vectorize the example\n",
    "2. Use the LDA on the vectorized example to predict the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [\"My team performed poorly last season. Their best player was out injured and only played one game\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04840481, 0.04840479, 0.80638069, 0.0484048 , 0.04840491]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_example=preprocessing(example[0])\n",
    "vectorized_example=vectorizer.transform([clean_example])\n",
    "lda_model.transform(vectorized_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "18340950b2391097feb7863714d098522e8885bd92dfaef7a735adaddffab167"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
